---
title: "Practical Machine Learning - Course Project"
author: "Chris Eagle"
date: "14 September 2015"
output: html_document
---
```{r echo=FALSE}
setwd("/Users/chriseagle/Documents/R/Practical Machine Learning/")
```

#Introduction
The goal of the project is to correctly predict the way in which a barbell is lifted, correctly and incorrectly using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. 
Details of the Weight Lifting Exercise can be found here http://groupware.les.inf.puc-rio.br/har.

#Setup
Training and test data were downloaded from the following locations and saved to the working directory.
The training data: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data: 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Loading the caret package, enabling multi-core processing and setting the seed: 
```{r}
library(caret)
library(doParallel)
cl<-makeCluster(detectCores())
registerDoParallel(cl)
set.seed(62334)
```
Reading in the training data set and split into 75% training data, 25% validation data set, to allow for repeated cross validation before final application to test data.
```{r}
training<-read.csv("pml-training.csv",na.strings=c("NA","#DIV/0!"))
TrainIndex<-createDataPartition(training$classe,p=0.75)[[1]]
training<-training[TrainIndex,]
validation<-training[-TrainIndex,]
```

#Feature Selection
The dataset provided has 160 variables, including details of the participant and time windows where the data was captured. In addition, there is the raw output from each of the accelerometers, and related summary statistics (e.g. max, min, st dev.).
If the raw data is to be included, then the summary statistics are effectively a 'double count' - they can be observed from the raw data. e.g. the 'max' value will be included as one of the raw data points. Secondly, the summary data is, logically, less numerous than the raw data, so not enough data points to train on. Hence, summary statistics excluded.

```{r}
col_names<-names(training)
exclude<-c("X","user_name","timestamp","window","kurtosis","skewness","min",
           "max","amplitude","var","stddev","avg")

exclude_cols<-NULL
for (i in exclude){
  exclude_cols<-c(exclude_cols,grep(i,col_names))
}

training<-training[-exclude_cols]
validation<-validation[-exclude_cols]
```

#Model Fitting
Caret's trainControl set to 5-fold cross validation repeated 5 times. This gave a reasonable performance/thoroughness tradeoff: gbm models training in 2-3 mins, rfs trained in 5mins.
```{r}
tc<-trainControl(method='cv',number=5,repeats=5)
```
For comparison, I fitted both 'boosting' and 'random forest' models and compared the accuracy.

Boosted Tree:
```{r}
ModelFit_gbm<-train(classe~.,method="gbm",data=training,trControl=tc)
prediction_gbm<-predict(ModelFit_gbm,validation)
confusionMatrix(validation$classe,prediction_gbm)
```

Random Forest:
```{r}
ModelFit_rf<-train(classe~.,method="rf",data=training,trControl=tc)
prediction_rf<-predict(ModelFit_rf,validation)
confusionMatrix(validation$classe,prediction_rf)
```

#Model Selection and Error Estimation
The Random Forest Model has a slightly higher accuracy than the Boosted Tree, so this is selected as the final model.  
```{r}
OOB1<-round(ModelFit_rf$finalModel[4][[1]][1]*100,2)
OOB500<-round(ModelFit_rf$finalModel[4][[1]][500]*100,2)
```
The out of bag error estimate goes from `r OOB1` for the first tree, to `r OOB500` for 500 trees.

#Variable Importance
"Roll_belt" scores highest in terms of variable importance:
```{r}
variable_imp<-varImp(ModelFit_rf)
plot(variable_imp,top=20)
```

#Predicting against the Test Set
The Random Forest Model is then used to predict against the test dataset.
```{r}
testing<-read.csv("pml-testing.csv")

answers<-predict(ModelFit_rf,testing)
```

#Export
The results are then exported using the following code.
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```

